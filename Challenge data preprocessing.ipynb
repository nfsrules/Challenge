{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-preprocessing notebook\n",
    "\n",
    "    1) Create a new TARGET variable (0 user stayed, 1 user leaved). Check consitency and fix error (if exists)\n",
    "    2) Drop all non-numerical variables (no time for encoding now. Leave it for another time)\n",
    "    3) Create a new DELTA_LAST_INVOICE_LOAD: Time from the last money load in reference to current invoice\n",
    "    4) Normalize datetime object in reference to account creation date\n",
    "    5) Filter variables and users that have more than 500 NaNs or NaTs    \n",
    "    \n",
    "    Author: Nelson Fernandez Pinto\n",
    "            @nfsrules\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('interview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CHURN_DATE                             4793\n",
       "LAST_FUNDRAISING_AMOUNT                4572\n",
       "CHURNING_DATE                          4499\n",
       "AMOUNT_SMI_REQ_EUR                     4289\n",
       "NB_SMI_REQ                             4289\n",
       "NUMBER_EXPORT                          3525\n",
       "ACCOUNTING_EXPORTS_GENERATED           3525\n",
       "MAIN_ENTITY                            3112\n",
       "AMOUNT_EXP_CL_REQ_EUR                  3093\n",
       "NB_EXP_CL_REQ                          3093\n",
       "SUM_TICKETS_JIRA                       2827\n",
       "NUMBER_OF_TEAMS_ACTIVE                 2774\n",
       "SUB_SEGMENT                            2500\n",
       "NPS                                    2374\n",
       "AVG_SCORE                              2162\n",
       "NB_SUBSCRIPTIONS_PAYMENTS              2080\n",
       "AMOUNT_SUBSCRIPTIONS_PAYMENTS_EUR      2080\n",
       "AMOUNT_PLASTIC_CARD_PAYMENTS_EUR       2027\n",
       "NB_PLASTIC_CARD_PAYMENTS               2027\n",
       "INDUSTRY                               1388\n",
       "REQUESTER_LOGIN_PER_USER               1385\n",
       "IS_FLAGSHIP                            1300\n",
       "NB_SINGLE_PURCHASE_PAYMENTS            1258\n",
       "AMOUNT_SINGLE_PURCHASE_PAYMENTS_EUR    1258\n",
       "PLAN                                   1194\n",
       "AVERAGE_TIME_PER_USER_REQUESTER        1076\n",
       "BUNDLE                                  887\n",
       "PAY_PER_LOAD                            820\n",
       "SUM_TICKETS                             802\n",
       "NON_REQUESTER_LOGIN_PER_USER            728\n",
       "LOGIN_PER_USER                          721\n",
       "HEALTHSCORE                             634\n",
       "AVERAGE_TIME_PER_USER_NON_REQUESTER     632\n",
       "HIGH_LOW_TOUCH                          599\n",
       "ACTIVE_USERS_PER_USERS                  556\n",
       "LAST_LOAD_DATE                          493\n",
       "ACTIVE_USERS_PER_FTE                    474\n",
       "COUNT_PAY_PER_ACTIVE_USERS              474\n",
       "AMOUNT_PAY_PER_ACTIVE_USERS             474\n",
       "CS_OWNER                                216\n",
       "AGG_INDUSTRY                            216\n",
       "AVERAGE_TIME_PER_USER                   168\n",
       "VAR_MRR_PER_FTE                         140\n",
       "VAR_MRR                                 140\n",
       "FIX_MRR                                 140\n",
       "USERS_PER_FTE                           109\n",
       "COUNTRY                                   0\n",
       "ORGANISATION_ID                           0\n",
       "NB_ACTIVE_PLASTIC_CARDS                   0\n",
       "FIRST_LOAD_DATE                           0\n",
       "SEGMENT                                   0\n",
       "ORGANISATION_NAME                         0\n",
       "AGE                                       0\n",
       "NAME                                      0\n",
       "FTE_AGG                                   0\n",
       "MONTH                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nans and nats\n",
    "df.groupby('ENTITY_ID').max().isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(date):\n",
    "    '''Check if churn rate date is NaT.\n",
    "    Input: Datetime or number\n",
    "    Returns:\n",
    "        0 if date is NaT (still client)\n",
    "        1 if date is not NaT (client left the company)\n",
    "    '''\n",
    "    return int(date != date)*-1+1\n",
    "\n",
    "\n",
    "def isNaN(num):\n",
    "    '''Check if num is NaN.\n",
    "    Input: Number\n",
    "    Returns:\n",
    "        True if value is NaN\n",
    "        False if value is not NaN\n",
    "    \n",
    "    '''\n",
    "    return num != num\n",
    "\n",
    "\n",
    "def check_churn_target(group, max_date):\n",
    "    '''Complete missings churn dates.\n",
    "    Input: Group (Pandas Dataframe of 1 user)\n",
    "    Returns:\n",
    "        Pandas dataframe of group with missing churn date added.\n",
    "        New column TARGET (1: user leaved, 0: user stayerd)\n",
    "    '''\n",
    "    # Sort rows by date\n",
    "    group.sort_values(by='MONTH', inplace=True)\n",
    "    \n",
    "    # Complete NaNs of churnig date (IN TOTAL 273 USERS)\n",
    "    if not group.MONTH.max() == max_date: # the client already exited Spendesk\n",
    "        if isNaN(group.CHURN_DATE.max()): # CHURN_DATE should not be NaN\n",
    "            group.CHURN_DATE = group.CHURNING_DATE.max() + relativedelta(months=1)\n",
    "    \n",
    "    # Add target inplace to group\n",
    "    group['TARGET'] = group.CHURN_DATE.apply(create_target)\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "def pre_processing_user(group, max_date):\n",
    "    '''\n",
    "    Create new variable 'DAYS_SINCE_LAST_LOAD'.\n",
    "    Time in days between last time money was load to the platform\n",
    "    in reference to last invoicing date.\n",
    "    \n",
    "    Normalize all dates in respecto to first invoice date.\n",
    "    \n",
    "    Input: User dataframe\n",
    "    Return: User dataframe\n",
    "    '''    \n",
    "    # Sort rows by date\n",
    "    group.sort_values(by='MONTH', inplace=True)\n",
    "    \n",
    "    # Complete NaNs of churnig date (IN TOTAL 273 USERS)\n",
    "    if not group.MONTH.max() == max_date: # the client already exited Spendesk\n",
    "        if isNaN(group.CHURN_DATE.max()): # CHURN_DATE should not be NaN\n",
    "            group.CHURN_DATE = group.CHURNING_DATE.max() + relativedelta(months=1)\n",
    "            \n",
    "    # Add target (0: Stays in platform, 1: Leave the platform)\n",
    "    group['TARGET'] = group.CHURN_DATE.apply(create_target)\n",
    "    # Time from last money load last invoice \n",
    "    group['DAYS_SINCE_LAST_LOAD'] = (group.MONTH.max() - group.LAST_LOAD_DATE).dt.days\n",
    "    # Number of days to the first money load\n",
    "    group.FIRST_LOAD_DATE = (group.FIRST_LOAD_DATE - group.MONTH.iloc[0]).dt.days\n",
    "    # Number of days to the last money load\n",
    "    group.LAST_LOAD_DATE = (group.LAST_LOAD_DATE - group.MONTH.iloc[0]).dt.days\n",
    "    # Normalize months\n",
    "    group.MONTH = (group.MONTH - group.MONTH.iloc[0]).dt.days\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "def filter_nan(df_users, max_nan_allowed=500, verbose=True):\n",
    "    '''Two-step NaN filtering.\n",
    "    Input\n",
    "        df: Pandas dataframe\n",
    "        max_nan_allowed: Mas number of user with nans allowed by variable\n",
    "        verbose: Print selected variables, default True\n",
    "        \n",
    "    Returns\n",
    "        List of selected variables\n",
    "        Pandas dataframe with selected variables\n",
    "    '''\n",
    "    # Get variable names that has less than 500 NaN users (first level filtering)\n",
    "    var_names = df_users.columns.values\n",
    "    \n",
    "    # Create a boolean mask with variables that have less than 500 users with NaN values\n",
    "    mask = (df_users.groupby('ENTITY_ID').max().isna().sum() < max_nan_allowed).values\n",
    "    mask = np.insert(mask, 0, True, axis=0)\n",
    "    selected_variables = var_names[mask]\n",
    "    \n",
    "    # Drop users that have NaNs on the pre-selected features (Second-level filtering)\n",
    "    selected_users = df_users[selected_variables].groupby('ENTITY_ID').max().dropna().index \n",
    "    \n",
    "    # Filter dataframe \n",
    "    df_filtered = df_users[selected_variables]\n",
    "    filtered_df = pd.concat([df_filtered[df_filtered.ENTITY_ID == name] for name in selected_users])\n",
    "    \n",
    "    if verbose:\n",
    "        print('Selected variables: ',selected_variables)\n",
    "        print('The filtered dataset has {} and {} variables'.format(len(selected_users),\n",
    "                                                            len(selected_variables)))\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def pre_processing_dataset(df, max_nan_allowed=500, verbose=True):\n",
    "    '''Pre-process dataframe:\n",
    "    Add new variable DAYS_SINCE_LAST_LOAD: Time from last money load\n",
    "    Normalize date variables respecting to first invoincing date.\n",
    "    Drop all variables with more than max_nan_allowed users\n",
    "    \n",
    "    Input: Pandas Dataframe\n",
    "    Returs: Pandas DataFrame\n",
    "    '''\n",
    "    # Convert date variables to datetime type\n",
    "    df.MONTH = pd.to_datetime(df.MONTH)\n",
    "    df.FIRST_LOAD_DATE = pd.to_datetime(df.FIRST_LOAD_DATE)\n",
    "    df.CHURN_DATE = pd.to_datetime(df.CHURN_DATE) \n",
    "    df.CHURNING_DATE = pd.to_datetime(df.CHURNING_DATE) \n",
    "\n",
    "    # Assuming all dates are in the same datetime\n",
    "    df.LAST_LOAD_DATE = pd.to_datetime(df.LAST_LOAD_DATE)\n",
    "    df.LAST_LOAD_DATE = df.LAST_LOAD_DATE.apply(lambda x: x.replace(tzinfo=None)) # forget timezone info\n",
    "\n",
    "    # The date of creation of the database is\n",
    "    max_date = df.MONTH.max()\n",
    "    # Pre-processing and add target to dataframe user per user\n",
    "    df_users = df.groupby('ENTITY_ID').apply(lambda x: pre_processing_user(x, max_date))\n",
    "    df_users = df_users.drop(columns=['ENTITY_ID']).reset_index().drop(columns=['level_1']) # back to original df shape\n",
    "    \n",
    "    # Filter Nans and NaTs\n",
    "    df_filtered = filter_nan(df_users, \n",
    "                              max_nan_allowed=max_nan_allowed,\n",
    "                              verbose=verbose)    \n",
    "    return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variables:  ['ENTITY_ID' 'MONTH' 'ORGANISATION_ID' 'COUNTRY' 'FIRST_LOAD_DATE'\n",
      " 'LAST_LOAD_DATE' 'ORGANISATION_NAME' 'CS_OWNER' 'NAME' 'FTE_AGG'\n",
      " 'AGG_INDUSTRY' 'AGE' 'SEGMENT' 'AVERAGE_TIME_PER_USER'\n",
      " 'AMOUNT_PAY_PER_ACTIVE_USERS' 'COUNT_PAY_PER_ACTIVE_USERS'\n",
      " 'ACTIVE_USERS_PER_FTE' 'USERS_PER_FTE' 'VAR_MRR_PER_FTE' 'VAR_MRR'\n",
      " 'FIX_MRR' 'NB_ACTIVE_PLASTIC_CARDS' 'TARGET' 'DELTA_LAST_INVOICE_LOAD']\n",
      "The filtered dataset has 4760 and 24 variables\n"
     ]
    }
   ],
   "source": [
    "df_filtered = pre_processing_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "#df_filterd.to_csv('filtered_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorch",
   "language": "python",
   "name": "newpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
